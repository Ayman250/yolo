{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "\n",
    "class DetectionTrainer:\n",
    "    def __init__(self, data_yaml_path):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.model = YOLO('yolov8n.pt')\n",
    "        self.data_yaml_path = str(Path(data_yaml_path).absolute())\n",
    "        \n",
    "        # Define separate transforms for each augmentation type\n",
    "        self.transforms_with_boxes = {\n",
    "            'brightness': A.Compose([\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.0, p=1.0),\n",
    "                A.Resize(640, 640),\n",
    "            ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'])),\n",
    "            \n",
    "            'contrast': A.Compose([\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.0, contrast_limit=0.3, p=1.0),\n",
    "                A.Resize(640, 640),\n",
    "            ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'])),\n",
    "            \n",
    "            'gamma': A.Compose([\n",
    "                A.RandomGamma(gamma_limit=(80, 120), p=1.0),\n",
    "                A.Resize(640, 640),\n",
    "            ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'])),\n",
    "            \n",
    "            'noise': A.Compose([\n",
    "                A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
    "                A.Resize(640, 640),\n",
    "            ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'])),\n",
    "            \n",
    "            'blur': A.Compose([\n",
    "                A.GaussianBlur(blur_limit=(3, 7), p=1.0),\n",
    "                A.Resize(640, 640),\n",
    "            ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'])),\n",
    "            \n",
    "            'rotation': A.Compose([\n",
    "                A.RandomRotate90(p=1.0),\n",
    "                A.Resize(640, 640),\n",
    "            ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'])),\n",
    "            \n",
    "            'flip': A.Compose([\n",
    "                A.Flip(p=1.0),\n",
    "                A.Resize(640, 640),\n",
    "            ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'])),\n",
    "        }\n",
    "        \n",
    "        # Same transforms without bbox params for images without boxes\n",
    "        self.transforms_no_boxes = {\n",
    "            name: A.Compose([t for t in transform if not isinstance(t, A.Resize)] + [A.Resize(640, 640)])\n",
    "            for name, transform in self.transforms_with_boxes.items()\n",
    "        }\n",
    "\n",
    "    def process_single_image(self, img_path, label_path, dest_dir):\n",
    "        \"\"\"Process a single image creating separate augmented versions for each transform\"\"\"\n",
    "        # Read image\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Read labels if they exist\n",
    "        boxes = []\n",
    "        class_labels = []\n",
    "        \n",
    "        if label_path.exists():\n",
    "            with open(label_path) as f:\n",
    "                for line in f.readlines():\n",
    "                    class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                    boxes.append([x_center, y_center, width, height])\n",
    "                    class_labels.append(class_id)\n",
    "\n",
    "        # Save original image\n",
    "        orig_img_path = dest_dir / 'images' / img_path.name\n",
    "        cv2.imwrite(str(orig_img_path), cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # Copy original label if it exists\n",
    "        if label_path.exists():\n",
    "            shutil.copy2(label_path, dest_dir / 'labels' / label_path.name)\n",
    "\n",
    "        # Apply each transform separately\n",
    "        transforms = self.transforms_with_boxes if boxes else self.transforms_no_boxes\n",
    "        \n",
    "        for transform_name, transform in transforms.items():\n",
    "            # Create augmented version with specific transform name\n",
    "            aug_img_name = f\"{img_path.stem}_{transform_name}{img_path.suffix}\"\n",
    "            aug_label_name = f\"{img_path.stem}_{transform_name}.txt\"\n",
    "            \n",
    "            if boxes:\n",
    "                transformed = transform(\n",
    "                    image=image,\n",
    "                    bboxes=boxes,\n",
    "                    class_labels=class_labels\n",
    "                )\n",
    "                transformed_image = transformed['image']\n",
    "                transformed_boxes = transformed['bboxes']\n",
    "                transformed_labels = transformed['class_labels']\n",
    "                \n",
    "                # Save augmented image\n",
    "                cv2.imwrite(\n",
    "                    str(dest_dir / 'images' / aug_img_name),\n",
    "                    cv2.cvtColor(transformed_image, cv2.COLOR_RGB2BGR)\n",
    "                )\n",
    "                \n",
    "                # Save augmented labels\n",
    "                with open(dest_dir / 'labels' / aug_label_name, 'w') as f:\n",
    "                    for box, label in zip(transformed_boxes, transformed_labels):\n",
    "                        f.write(f\"{int(label)} {' '.join(map(str, box))}\\n\")\n",
    "            else:\n",
    "                transformed = transform(image=image)\n",
    "                transformed_image = transformed['image']\n",
    "                cv2.imwrite(\n",
    "                    str(dest_dir / 'images' / aug_img_name),\n",
    "                    cv2.cvtColor(transformed_image, cv2.COLOR_RGB2BGR)\n",
    "                )\n",
    "                \n",
    "                if label_path.exists():\n",
    "                    shutil.copy2(label_path, dest_dir / 'labels' / aug_label_name)\n",
    "\n",
    "    def prepare_split_datasets(self, source_dir, train_dir, val_dir, val_split=0.2):\n",
    "        \"\"\"Split dataset and prepare train/val directories with separate augmentations\"\"\"\n",
    "        source_dir = Path(source_dir)\n",
    "        train_dir = Path(train_dir)\n",
    "        val_dir = Path(val_dir)\n",
    "        \n",
    "        # Create necessary directories\n",
    "        for dir_path in [train_dir / 'images', train_dir / 'labels',\n",
    "                        val_dir / 'images', val_dir / 'labels']:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get all image paths\n",
    "        image_files = list((source_dir / 'images').glob('*.jpg'))\n",
    "        \n",
    "        # Split into train and val\n",
    "        train_imgs, val_imgs = train_test_split(\n",
    "            image_files,\n",
    "            test_size=val_split,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"Processing {len(train_imgs)} training images...\")\n",
    "        for img_path in train_imgs:\n",
    "            label_path = source_dir / 'labels' / img_path.with_suffix('.txt').name\n",
    "            self.process_single_image(img_path, label_path, train_dir)\n",
    "            \n",
    "        print(f\"Processing {len(val_imgs)} validation images...\")\n",
    "        for img_path in val_imgs:\n",
    "            label_path = source_dir / 'labels' / img_path.with_suffix('.txt').name\n",
    "            # For validation, we only copy original images without augmentation\n",
    "            shutil.copy2(img_path, val_dir / 'images' / img_path.name)\n",
    "            if label_path.exists():\n",
    "                shutil.copy2(label_path, val_dir / 'labels' / label_path.name)\n",
    "        \n",
    "        print(f\"Processing {len(train_imgs)} training images...\")\n",
    "        for img_path in train_imgs:\n",
    "            label_path = source_dir / 'labels' / img_path.with_suffix('.txt').name\n",
    "            self.process_single_image(img_path, label_path, train_dir)\n",
    "            \n",
    "        print(f\"Processing {len(val_imgs)} validation images...\")\n",
    "        for img_path in val_imgs:\n",
    "            label_path = source_dir / 'labels' / img_path.with_suffix('.txt').name\n",
    "            # For validation, we only copy original images without augmentation\n",
    "            shutil.copy2(img_path, val_dir / 'images' / img_path.name)\n",
    "            if label_path.exists():\n",
    "                shutil.copy2(label_path, val_dir / 'labels' / label_path.name)\n",
    "\n",
    "    def train(self, epochs=20, imgsz=640, batch_size=128, val_split=0.2):\n",
    "        \"\"\"Train the model using YOLO's training pipeline\"\"\"\n",
    "        print(f\"Training on {self.device}\")\n",
    "        \n",
    "        # Create directories for split datasets\n",
    "        train_dir = Path('split_train')\n",
    "        val_dir = Path('split_val')\n",
    "        \n",
    "        # Clean up any existing directories\n",
    "        for dir_path in [train_dir, val_dir]:\n",
    "            if dir_path.exists():\n",
    "                shutil.rmtree(dir_path)\n",
    "        \n",
    "        # Prepare split datasets\n",
    "        print(\"Preparing train/val splits and augmentations...\")\n",
    "        self.prepare_split_datasets('train', train_dir, val_dir, val_split)\n",
    "        \n",
    "        # Update data.yaml with split paths\n",
    "        with open(self.data_yaml_path, 'r') as f:\n",
    "            data_config = yaml.safe_load(f)\n",
    "        \n",
    "        data_config['train'] = str(train_dir / 'images')\n",
    "        data_config['val'] = str(val_dir / 'images')\n",
    "        \n",
    "        temp_yaml_path = 'temp_data.yaml'\n",
    "        with open(temp_yaml_path, 'w') as f:\n",
    "            yaml.dump(data_config, f)\n",
    "        \n",
    "        # Training arguments\n",
    "        args = dict(\n",
    "            data=temp_yaml_path,\n",
    "            epochs=epochs,\n",
    "            imgsz=imgsz,\n",
    "            batch=batch_size,\n",
    "            patience=20,\n",
    "            save_period=10,\n",
    "            verbose=True,\n",
    "            device=0 if self.device.type == 'cuda' else 'cpu',\n",
    "            project=str(Path().absolute() / 'runs'),\n",
    "            augment=True\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Train the model\n",
    "            self.model.train(**args)\n",
    "        finally:\n",
    "            # Cleanup\n",
    "            for dir_path in [train_dir, val_dir]:\n",
    "                if dir_path.exists():\n",
    "                    shutil.rmtree(dir_path)\n",
    "            if os.path.exists(temp_yaml_path):\n",
    "                os.remove(temp_yaml_path)\n",
    "\n",
    "    def test(self, conf_threshold=0.25, iou_threshold=0.45):\n",
    "        \"\"\"Test the model on the test dataset\"\"\"\n",
    "        print(f\"\\nRunning tests on {self.device}\")\n",
    "        \n",
    "        # Create a temporary yaml file for testing\n",
    "        with open(self.data_yaml_path, 'r') as f:\n",
    "            data_config = yaml.safe_load(f)\n",
    "        \n",
    "        # Update paths to point to test directory\n",
    "        data_config['val'] = str(Path('test/images'))  # Use test directory for validation\n",
    "        \n",
    "        temp_yaml_path = 'temp_test_data.yaml'\n",
    "        with open(temp_yaml_path, 'w') as f:\n",
    "            yaml.dump(data_config, f)\n",
    "            \n",
    "        try:\n",
    "            results = self.model.val(\n",
    "                data=temp_yaml_path,\n",
    "                split='test',\n",
    "                conf=conf_threshold,\n",
    "                iou=iou_threshold,\n",
    "                device=0 if self.device.type == 'cuda' else 'cpu',\n",
    "                verbose=True\n",
    "            )\n",
    "            return results\n",
    "        finally:\n",
    "            # Clean up temporary yaml\n",
    "            if os.path.exists(temp_yaml_path):\n",
    "                os.remove(temp_yaml_path)\n",
    "\n",
    "    def predict(self, image_path):\n",
    "        \"\"\"Run inference on a single image\"\"\"\n",
    "        return self.model.predict(\n",
    "            source=image_path,\n",
    "            conf=0.25,\n",
    "            iou=0.45,\n",
    "            device=0 if self.device.type == 'cuda' else 'cpu'\n",
    "        )\n",
    "\n",
    "def prepare_dataset_structure():\n",
    "    \"\"\"\n",
    "    Create the simplified dataset structure\n",
    "    \"\"\"\n",
    "    dirs = ['train/images', 'train/labels', \n",
    "            'test/images', 'test/labels']\n",
    "    \n",
    "    for dir_path in dirs:\n",
    "        Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def create_data_yaml(dataset_path):\n",
    "    \"\"\"\n",
    "    Create the data.yaml file for YOLOv8\n",
    "    \"\"\"\n",
    "    # Convert to absolute path\n",
    "    abs_path = str(Path(dataset_path).absolute())\n",
    "    \n",
    "    yaml_content = f\"\"\"\n",
    "path: {abs_path}  # dataset root directory\n",
    "train: {abs_path}/train/images  # train images\n",
    "val: {abs_path}/temp_val/images  # temporary validation images directory\n",
    "test: {abs_path}/test/images    # test images\n",
    "\n",
    "# Classes\n",
    "names:\n",
    "  0: negative\n",
    "  1: positive\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('data.yaml', 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "\n",
    "def main():\n",
    "    # Get absolute path of current directory\n",
    "    current_dir = str(Path().absolute())\n",
    "    \n",
    "    # Create dataset structure\n",
    "    prepare_dataset_structure()\n",
    "    \n",
    "    # Create data.yaml with absolute paths\n",
    "    create_data_yaml(current_dir)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = DetectionTrainer('data.yaml')\n",
    "    \n",
    "    # Train model\n",
    "    trainer.train(epochs=3, val_split=0.2)\n",
    "    \n",
    "    # Run tests and get results\n",
    "    test_results = trainer.test(conf_threshold=0.25, iou_threshold=0.45)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
